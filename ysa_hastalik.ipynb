{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c28765fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from collections import Counter\n",
    "\n",
    "import kagglehub\n",
    "import shutil\n",
    "import os\n",
    "\n",
    "import shutil\n",
    "import math\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms, models\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "# from sklearn.metrics import classification_report\n",
    "# import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65094a2a",
   "metadata": {},
   "source": [
    "Veriler ile ilgili bilgi edindiğimiz kısım"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "97c68294",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Toplam 15 farklı hastalık etiketi var.\n",
      "Hastalık etiketleri (çoktan aza doğru):\n",
      "No Finding: 60361\n",
      "Infiltration: 9547\n",
      "Atelectasis: 4215\n",
      "Effusion: 3955\n",
      "Nodule: 2705\n",
      "Pneumothorax: 2194\n",
      "Mass: 2139\n",
      "Consolidation: 1310\n",
      "Pleural_Thickening: 1126\n",
      "Cardiomegaly: 1093\n",
      "Emphysema: 892\n",
      "Fibrosis: 727\n",
      "Edema: 628\n",
      "Pneumonia: 322\n",
      "Hernia: 110\n"
     ]
    }
   ],
   "source": [
    "# CSV dosyasını oku\n",
    "csv_path = \"Data_Entry_2017.csv\"\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "\n",
    "# Sadece tek bir etiketi olanları filtrele\n",
    "single_label_df = df[df[\"Finding Labels\"].str.contains(r\"\\|\", na=False) == False]\n",
    "\n",
    "# Hastalık etiketlerini say\n",
    "label_counts = Counter(single_label_df[\"Finding Labels\"].dropna())  # Boş olmayanları al\n",
    "\n",
    "\n",
    "\n",
    "# # Birden çok hastalık etiketlerini de say \n",
    "# label_counts = Counter()\n",
    "# for labels in df[\"Finding Labels\"].dropna():  # Boş olmayanları al\n",
    "#     label_counts.update(labels.split(\"|\"))  # \"|\" ile ayrılanları ayır ve say\n",
    "\n",
    "# Etiketleri çoktan aza doğru sırala\n",
    "sorted_labels = label_counts.most_common()\n",
    "\n",
    "# Sonuçları yazdır\n",
    "print(f\"Toplam {len(label_counts)} farklı hastalık etiketi var.\")\n",
    "print(\"Hastalık etiketleri (çoktan aza doğru):\")\n",
    "for label, count in sorted_labels:\n",
    "    print(f\"{label}: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12f865ab",
   "metadata": {},
   "source": [
    "Verileri Türlerine göre ayırdığımız kısım"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b87e25ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def organize_images(df, source_folder = \"archive/\", destination_folder= \"dataset/\", chose = 1):\n",
    "    \"\"\"\n",
    "    Görüntüleri etiketlere göre organize eden bir fonksiyon.\n",
    "\n",
    "    Bu fonksiyon, verilen bir DataFrame'deki görüntü dosyalarını etiketlerine göre\n",
    "    belirtilen hedef klasöre taşır veya kopyalar. Ayrıca, tüm görüntüleri tek bir\n",
    "    klasöre toplama seçeneği sunar.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): Görüntü dosyalarının bilgilerini içeren DataFrame.\n",
    "                           \"Image Index\" sütunu dosya adlarını, \"Finding Labels\" sütunu ise etiketleri içermelidir.\n",
    "        source_folder (str): Kaynak klasörün yolu. Varsayılan olarak \"archive/\".\n",
    "        destination_folder (str): Hedef klasörün yolu. Varsayılan olarak \"dataset/\".\n",
    "        chose (int): İşlem türünü belirler:\n",
    "                     - 1: Görüntüleri etiketlerine göre ayrı klasörlere taşır/kopyalar.\n",
    "                     - 0: Tüm görüntüleri tek bir klasöre taşır/kopyalar.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "\n",
    "    Örnek:\n",
    "        Tek etiketli görüntüleri organize etmek için:\n",
    "        >>> organize_images(single_label_df, source_folder=\"archive/\", destination_folder=\"dataset/\", chose=1)\n",
    "    \"\"\"\n",
    "\n",
    "    if chose == 0:\n",
    "        # Genel hedef klasör (eğer chose = 0 ise tüm görüntüler buraya gider)\n",
    "        general_folder = os.path.join(destination_folder, \"single_label_images\")\n",
    "\n",
    "    for folder in os.listdir(source_folder):\n",
    "        images_path = os.path.join(source_folder, folder, \"images\")\n",
    "\n",
    "        if os.path.exists(images_path):  # Eğer images klasörü varsa\n",
    "\n",
    "            for filename in os.listdir(images_path):  # İçindeki tüm görüntüleri tarayalım\n",
    "                file_path = os.path.join(images_path, filename)  # Dosyanın tam yolu\n",
    "                \n",
    "\n",
    "                # CSV'den uygun etiket bulunuyor\n",
    "                row = df.loc[df[\"Image Index\"] == filename]  \n",
    "                if not row.empty:\n",
    "                    \n",
    "                    if chose == 1:\n",
    "                        label = row[\"Finding Labels\"].values[0]  # Hastalık etiketi\n",
    "\n",
    "                        # Etikete göre hedef klasör oluştur\n",
    "                        label_folder = os.path.join(destination_folder, label)\n",
    "                        os.makedirs(label_folder, exist_ok=True)\n",
    "\n",
    "                        # Dosyayı taşı\n",
    "                        destination_path = os.path.join(label_folder, filename)\n",
    "                        shutil.copy(file_path, destination_path)\n",
    "\n",
    "\n",
    "                    elif chose == 0:\n",
    "                        os.makedirs(general_folder, exist_ok=True)\n",
    "\n",
    "                        # Tüm görüntüleri tek klasöre kopyala\n",
    "                        destination_path = os.path.join(general_folder, filename)\n",
    "                        shutil.copy(file_path, destination_path)\n",
    "\n",
    "\n",
    "    print(\"Tüm görüntüler etiketlere göre başarıyla taşındı!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5bbab9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tüm görüntüler etiketlere göre başarıyla taşındı!\n"
     ]
    }
   ],
   "source": [
    "# Tek etiketli görüntüleri organize et\n",
    "organize_images(single_label_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "42634300",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tüm görüntüler etiketlere göre başarıyla taşındı!\n"
     ]
    }
   ],
   "source": [
    "# Tek etiketli görüntüleri tek bir klasöre kopyala\n",
    "organize_images(single_label_df, destination_folder= \"single_dataset/\", chose= 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e9b77a35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def egitim_dosya_hazirla(source_folder= 'dataset/',\n",
    "                         destination_folder= \"data_model/\",\n",
    "                         excluded_folder = 'No Finding',\n",
    "                         num_images_per_class = 1000,\n",
    "                         train_split_ratio = 0.85,\n",
    "                         test_split_ratio = 0.10\n",
    "                         ):\n",
    "    \n",
    "\n",
    "    os.makedirs(destination_folder, exist_ok=True)\n",
    "    disease_folders = [d for d in os.listdir(source_folder)]\n",
    "\n",
    "\n",
    "\n",
    "    for disease_name in disease_folders:\n",
    "        # Hariç tutulan klasörü atla\n",
    "        if disease_name == excluded_folder:\n",
    "            continue\n",
    "\n",
    "        disease_path = os.path.join(source_folder, disease_name)\n",
    "\n",
    "        all_images = [f for f in os.listdir(disease_path)]\n",
    "\n",
    "        num_available_images = len(all_images)\n",
    "        if num_available_images < num_images_per_class:\n",
    "            print(f\" Uyarı: '{disease_name}' klasöründe yeterli resim yok ({num_available_images} < {num_images_per_class}). Bu sınıf atlanıyor.\")\n",
    "            continue\n",
    "\n",
    "        # Rastgele 1000 resim seç\n",
    "        selected_images = random.sample(all_images, num_images_per_class)  \n",
    "\n",
    "        # Seçilen resimleri karıştır\n",
    "        random.shuffle(selected_images)\n",
    "\n",
    "        # Bölme için resim sayılarını hesapla\n",
    "        num_train = int(num_images_per_class * train_split_ratio)\n",
    "        num_test = int(num_images_per_class * test_split_ratio)\n",
    "        # Küsüratları doğrulama setine ekleyerek tam sayıya tamamla\n",
    "        num_val = num_images_per_class - num_train - num_test\n",
    "\n",
    "        # Resimleri bölümlere ayır\n",
    "        train_images = selected_images[:num_train]\n",
    "        test_images = selected_images[num_train:num_train + num_test]\n",
    "        val_images = selected_images[num_train + num_test:]\n",
    "    \n",
    "        # Hedef klasörleri oluştur\n",
    "        split_mapping = {\n",
    "        'train': train_images,  # Eğitim seti\n",
    "        'validation': val_images, # Doğrulama seti\n",
    "        'test': test_images     # Test seti\n",
    "        }\n",
    "\n",
    "        for split_name, image_list in split_mapping.items():\n",
    "\n",
    "            dest_dir = os.path.join(destination_folder, split_name, disease_name)\n",
    "            os.makedirs(dest_dir, exist_ok=True) # İç içe klasörleri oluşturur\n",
    "\n",
    "            # Resimleri kopyalama işlemi\n",
    "            for image_filename in image_list:\n",
    "                source_file_path = os.path.join(disease_path, image_filename)\n",
    "                dest_file_path = os.path.join(dest_dir, image_filename)\n",
    "                \n",
    "                shutil.copy(source_file_path, dest_file_path) # Sadece dosyayı kopyalar\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "4a35ac34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Uyarı: 'Edema' klasöründe yeterli resim yok (628 < 1000). Bu sınıf atlanıyor.\n",
      " Uyarı: 'Emphysema' klasöründe yeterli resim yok (892 < 1000). Bu sınıf atlanıyor.\n",
      " Uyarı: 'Fibrosis' klasöründe yeterli resim yok (727 < 1000). Bu sınıf atlanıyor.\n",
      " Uyarı: 'Hernia' klasöründe yeterli resim yok (110 < 1000). Bu sınıf atlanıyor.\n",
      " Uyarı: 'Pneumonia' klasöründe yeterli resim yok (322 < 1000). Bu sınıf atlanıyor.\n"
     ]
    }
   ],
   "source": [
    "egitim_dosya_hazirla()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "bdf66e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.Grayscale(num_output_channels=1),\n",
    "        transforms.Resize((512, 512)),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.5], [0.5])  # grayscale için\n",
    "    ]),\n",
    "    'validation': transforms.Compose([\n",
    "        transforms.Grayscale(num_output_channels=1),\n",
    "        transforms.Resize((512, 512)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.5], [0.5])\n",
    "    ]),\n",
    "    'test': transforms.Compose([\n",
    "        transforms.Grayscale(num_output_channels=1),\n",
    "        transforms.Resize((512, 512)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.5], [0.5])\n",
    "    ])\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "c5dd410e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sınıflar: ['Atelectasis', 'Cardiomegaly', 'Consolidation', 'Effusion', 'Infiltration', 'Mass', 'Nodule', 'Pleural_Thickening', 'Pneumothorax']\n"
     ]
    }
   ],
   "source": [
    "data_dir = 'data_model/'\n",
    "\n",
    "image_datasets = {\n",
    "    x: datasets.ImageFolder(os.path.join(data_dir, x), data_transforms[x])\n",
    "    for x in ['train', 'validation', 'test']\n",
    "}\n",
    "\n",
    "dataloaders = {\n",
    "    x: DataLoader(image_datasets[x], batch_size=32, shuffle=True, num_workers=2)\n",
    "    for x in ['train', 'validation', 'test']\n",
    "}\n",
    "\n",
    "class_names = image_datasets['train'].classes\n",
    "print(\"Sınıflar:\", class_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "1a5ee7f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloaders = {\n",
    "    x: DataLoader(image_datasets[x], batch_size=16, shuffle=True, num_workers=2, pin_memory=True)\n",
    "    for x in ['train', 'validation', 'test']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "7b5592f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\keser\\miniconda3\\envs\\ysa_env\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\keser\\miniconda3\\envs\\ysa_env\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "model = models.resnet18(pretrained=True)\n",
    "\n",
    "# İlk katman RGB (3 kanal) bekliyor. 1 kanala uyarladık.\n",
    "model.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "\n",
    "# Son katmanı sınıf sayısına göre değiştirdik.\n",
    "num_ftrs = model.fc.in_features\n",
    "model.fc = nn.Linear(num_ftrs, len(class_names))\n",
    "\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "0044e503",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "aa15084b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, dataloaders, criterion, optimizer, device, num_epochs=10):\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f'\\nEpoch {epoch+1}/{num_epochs}')\n",
    "        print('-' * 20)\n",
    "\n",
    "        for phase in ['train', 'validation']:\n",
    "            if phase == 'train':\n",
    "                model.train()\n",
    "            else:\n",
    "                model.eval()\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "            epoch_loss = running_loss / len(dataloaders[phase].dataset)\n",
    "            epoch_acc = running_corrects.double() / len(dataloaders[phase].dataset)\n",
    "\n",
    "            print(f'{phase.capitalize()} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2652243",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/10\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "trained_model = train_model(model, dataloaders, criterion, optimizer, device, num_epochs=10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ysa_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
